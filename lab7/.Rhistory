pos %>% group_by(word2) %>%
count(pos_count, sort = T)
?summarise
pos <- top_bigrams %>% group_by(word2,pos1) %>%
summarise(pos_count=n(), total_count= sum(pos1),.groups = 'drop')
pos <- top_bigrams %>% group_by(word2,pos1) %>%
summarise(pos_count=n(), total_count= sum(pos_count),.groups = 'drop')
View(pos)
pos <- top_bigrams %>% group_by(word2,pos1) %>%
summarise(pos_count=n(), total_count= sum(n()),.groups = 'drop')
View(pos)
pos <- top_bigrams %>% group_by(word2,pos1) %>%
summarise(pos_count=n(),.groups = 'drop')
pos2 <- pos %>% group_by(word2) %>%
count(pos1, sort = T)
View(pos2)
pos2 <- pos %>% group_by(word2) %>%
summarise(total_count=n(),.groups = 'drop')
pos2 <- pos %>% group_by(word2) %>%
summarise(total_count=sum(),.groups = 'drop')
pos2 <- pos %>% group_by(word2) %>%
summarise(total_count=sum(pos1),.groups = 'drop')
pos2 <- pos %>% group_by(word2) %>%
summarise(total_count=sum(pos1),.groups = 'drop')
View(pos)
pos2 <- pos %>% group_by(word2, pos1) %>%
summarise(total_count=sum(),.groups = 'drop')
View(pos2)
pos2 <- pos %>% group_by(word2) %>%
summarise(total_count=sum(),.groups = 'drop')
pos2 <- pos %>% group_by(word2) %>%
summarise(total_count=sum(pos_count),.groups = 'drop')
pos <- top_bigrams %>% group_by(word2,pos1) %>%
summarise(pos_count=n(),.groups = 'drop') %>% left_join(top_bigrams, by = c("word2" = "word2"))
pos <- top_bigrams %>% group_by(word2,pos1) %>%
summarise(pos_count=n(),.groups = 'drop') %>% left_join(top_bigrams[["word2", "n"]], by = c("word2" = "word2"))
pos <- top_bigrams %>% group_by(word2,pos1) %>%
summarise(pos_count=n(),.groups = 'drop') %>% left_join(top_bigrams[["word2", "n"],], by = c("word2" = "word2"))
top_bigrams[["word2", "n"],:]
top_bigrams[["word2", "n"],]
top_bigrams[c("word2", "n"),]
select(top_bigrams, c("word2", "n"))
select(top_bigrams, c("word2", "n")) %>% group_by(word2)
pos <- top_bigrams %>% group_by(word2,pos1) %>%
summarise(pos_count=n(),.groups = 'drop')
pos2 <- pos %>% group_by(word2) %>%
summarise(total_count=sum(pos_count),.groups = 'drop')
pos3 <- pos %>% left_join(pos2, by = c("word2" = "word2"))
View(pos3)
pos3 <- pos %>% left_join(pos2, by = c("word2" = "word2")) %>%
mutate(nn = pos_count / total_count)
pos3 <- pos3[order(-n, nn),]
pos3 <- pos3[order(-total_count, nn),]
pos3 <- pos3[order(-total_count, nn),]
pos3 <- pos3[order(-total_count, -nn),]
pos3 <- pos3[order(total_count, -nn),]
pos3 <- pos3[order(-pos3$total_count, -pos3$nn),]
pos <- pos %>% left_join(pos2, by = c("word2" = "word2")) %>%
mutate(nn = pos_count / total_count)
pos <- pos3[order(-pos3$total_count, -pos3$nn),]
ggplot(pos) +
geom_bar(aes(x = reorder(pos1, -nn), nn, fill = reorder(pos1, nn)), stat="identity") +
coord_flip() + theme(legend.position = "none") +
labs(y = "fraction", x = "POS")
ggplot(pos) +
geom_bar(aes(x = reorder(pos1, -nn), nn, fill = reorder(pos1, nn)), stat="identity") +
coord_flip() + theme(legend.position = "none") +
labs(y = "fraction", x = "POS") + facet_wrap(~word2)
ggplot(pos) +
geom_bar(aes(x = reorder(pos1, -nn), nn, fill = reorder(pos1, nn)), stat="identity") +
coord_flip() + theme(legend.position = "none") +
labs(y = "fraction", x = "POS") + facet_wrap(~word2) + coord_flip()
source("C:/Users/bogum/Desktop/mgr_studia/semestr3/Text/zad5/zad5.R", echo=TRUE)
source("C:/Users/bogum/Desktop/mgr_studia/semestr3/Text/zad5/zad5.R", echo=TRUE)
source("C:/Users/bogum/Desktop/mgr_studia/semestr3/Text/zad5/zad5.R", echo=TRUE)
top_bigrams %>%
inner_join(get_sentiments("bing"), by = c("word1" = "word")) %>%
count(word1, sentiment, sort = TRUE) %>%
acast(word1 ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red", "darkgreen"), max.words = 100)
top_bigrams %>%
inner_join(get_sentiments("bing"), by = c("word1" = "word")) %>%
count(word1, sentiment, sort = TRUE) %>%
acast(word1 ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red", "darkgreen"), max.words = 100)
text(x=0, y=0, paste(top_nouns$word2[1],",", top_nouns$word2[2],",", top_nouns$word2[3]), cex = 0.5)
text(x=-1, y=0, paste(top_nouns$word2[1],",", top_nouns$word2[2],",", top_nouns$word2[3]), cex = 0.5)
text(x=-1, y=0, paste(top_nouns$word2[1],",", top_nouns$word2[2],",", top_nouns$word2[3]), cex = 0.5)
text(x=0, y=-0.5, paste(top_nouns$word2[1],",", top_nouns$word2[2],",", top_nouns$word2[3]), cex = 0.5)
top_bigrams_sentiments <- top_bigrams %>%
inner_join(get_sentiments("bing"), by = c("word1" = "word")) %>%
count(word1, sentiment, sort = TRUE) %>%
top_bigrams_sentiments <- top_bigrams %>%
inner_join(get_sentiments("bing"), by = c("word1" = "word")) %>%
count(word1, sentiment, sort = TRUE)
top_bigrams_sentiments <- top_bigrams %>%
inner_join(get_sentiments("bing"), by = c("word1" = "word")) %>%
count(word1, sentiment, sort = TRUE)
View(top_bigrams_sentiments)
top_bigrams_sentiments <- top_bigrams %>%
left_join(get_sentiments("bing"), by = c("word1" = "word")) %>%
count(word1, sentiment, sort = TRUE)
View(top_bigrams_sentiments)
top_bigrams_sentiments <- top_bigrams %>%
left_join(get_sentiments("bing"), by = c("word1" = "word"))
View(top_bigrams_sentiments)
View(top_bigrams_sentiments)
source("C:/Users/bogum/Desktop/mgr_studia/semestr3/Text/zad6/zad6.R", echo=TRUE)
data.bbc.equal %>%
group_by(emo) %>%
summarise(n = n())
dim(tdm); length(class)
tmp <- slim.tdm.matrix(tdm,data.bbc.equal, 5, 1)
tdm <- tmp[[1]]
class <- tmp[[2]]
dim(tdm); length(class)
model.equal <- make.svm.model(tdm, class)
tdm <- make.tdm.matrix(data.bbc.equal)
tmp <- slim.tdm.matrix(tdm,data.bbc.equal, 5, 1)
tdm <- tmp[[1]]
class <- tmp[[2]]
dim(tdm); length(class)
model.equal <- make.svm.model(tdm, class)
confusionMatrix(model.random)
confusionMatrix(model.equal)
res <- resamples(list(Random = model.random, Equal = model.equal))
bwplot(res)
tdm <- make.tdm.matrix(data.bbc.equal)
tmp <- slim.tdm.matrix(tdm,data.bbc.equal, 6, 1)
tdm <- tmp[[1]]
class <- tmp[[2]]
dim(tdm); length(class)
model.equal <- make.svm.model(tdm, class)
confusionMatrix(model.equal)
res <- resamples(list(Random = model.random, Equal = model.equal))
bwplot(res)
tdm <- make.tdm.matrix(data.bbc.equal)
tmp <- slim.tdm.matrix(tdm,data.bbc.equal, 4, 1)
tdm <- tmp[[1]]
class <- tmp[[2]]
dim(tdm); length(class)
model.equal <- make.svm.model(tdm, class)
res <- resamples(list(Random = model.random, Equal = model.equal))
bwplot(res)
library(dplyr)
library(tidytext)
library(tm)
library(lsa)
library(gutenbergr)
library(topicmodels)
library(ggplot2)
rm(list = ls())
g <- gutenberg_works()
verne_en <- gutenberg_works(languages = "en") %>%
filter(author == "Verne, Jules")
austen_en <- gutenberg_works(languages = "en") %>%
filter(author == "Austen, Jane")
verne_en_ids <- c(83, 103, 3526)
verne_en_books <- verne_en[verne_en$gutenberg_id %in% verne_en_ids,c("gutenberg_id","title", "author")]
austen_en_ids <- c(105, 121, 141)
austen_en_books <- austen_en[austen_en$gutenberg_id %in% austen_en_ids,c("gutenberg_id","title", "author")]
v <- gutenberg_download(c(verne_en_ids,austen_en_ids))
v <- left_join(v, rbind(verne_en_books, austen_en_books))
df <- v %>%
group_by(title, author) %>%
summarise(text = paste(text, collapse = " "))
removeSpecialChars <- function(x) gsub("[^0-9A-Za-z///' ]"," ",x)
docs_ids <- seq(1:6)
df$doc_id <- docs_ids
titles <- df$title
corpus_verne <- VCorpus(DataframeSource(as.data.frame(df[df$author == "Verne, Jules",])))
corpus_verne <- corpus_verne %>% tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(removeSpecialChars)) %>%
tm_map(removePunctuation) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
View(corpus_verne)
doc_verne <- TermDocumentMatrix(corpus_verne)
View(doc_verne)
txt <- c("asda224", "24asaf")
removeSpecialChars(txt)
removeSpecialChars <- function(x) gsub("[0-9A-Za-z///' ]"," ",x)
removeSpecialChars <- function(x) gsub("[0-9A-Za-z///' ]"," ",x)
removeSpecialChars(txt)
removeNumbers(txt)
removeNumbers(df$text)
View(df)
df$text<- removeNumbers(df$text)
titles <- df$title
View(df)
corpus_verne <- VCorpus(DataframeSource(as.data.frame(df[df$author == "Verne, Jules",])))
corpus_verne <- corpus_verne %>% tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(removeSpecialChars)) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
doc_verne <- TermDocumentMatrix(corpus_verne)
ap_lda <- LDA(doc_verne, k = 2, control = list(seed = 1234))
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") + scale_x_reordered() + coord_flip()
View(doc_verne)
doc_verne[["dimnames"]][["Terms"]]
df$text
corpus_verne <- VCorpus(DataframeSource(as.data.frame(df[df$author == "Verne, Jules",])))
corpus_verne <- corpus_verne %>% tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(removeSpecialChars)) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
doc_verne <- TermDocumentMatrix(corpus_verne)
View(doc_verne)
doc_verne[["dimnames"]][["Terms"]]
View(df)
doc_verne
df <- v %>%
group_by(title, author) %>%
summarise(text = paste(text, collapse = " "))
df$doc_id <- docs_ids
corpus_verne <- VCorpus(DataframeSource(as.data.frame(df[df$author == "Verne, Jules",])))
corpus_verne <- corpus_verne %>% tm_map(removeWords, stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(removeSpecialChars)) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
doc_verne <- TermDocumentMatrix(corpus_verne)
doc_verne
View(doc_verne)
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/lab4/wyklad_przyklad.R", echo=TRUE)
as.vector(phi.Y)
sum(phi.Y)
rep(phi.X, each =2)
?rep
rep(phi.X, times =2)
t(phi.X)
rep(t(phi.X), times =2)
rep(t(phi.X), times =2) * as.vector(phi.Y)%>%
array(dim = c(2,2,2,2)) -> phi.XY
View(phi.X)
phi.XY
phi.Y
rep(t(phi.X), times =2) * as.vector(phi.Y)%>%
array(dim = c(2,2,2,2), dimnames = list(c("b1", "b2"), c("a1","a2"), c("d1","d2"), c("c1", "c2"))) -> phi.XY
phi.XY
rep(t(phi.X), times =2) * as.vector(phi.Y)
rep(phi.Y, each =2)
rep(phi.X, each =4)
rep(phi.X, times =4)
rep(phi.X, each =4)
rep(phi.Y, times =2)
output.vec -> vector()
output.vec -> c()
output.vec -> c()
as.vector(phi.X)
c(c(1,2), c(3,4))
phi.Y$d1
phi.Y
phi.Y["c1"]
phi.Y["c1",]
phi.Y[,,"c1"]
phi.Y["c1",,]
phi.Y[,"d1","c1"]
rep(phi.Y[,"d1","c1"], times =2)
as.vector(phi.X)
as.vector(phi.X) * rep(phi.Y[,"d1","c1"], times =2)
dimnames(phi.Y)
dimnames(phi.Y)[2]
dimnames(phi.Y)[2, 3]
dimnames(phi.Y)[2, 2]
dimnames(phi.Y)[2]
dimnames(phi.Y)[2:3]
psi <- array(0, dim = c(2,2,2,2), dimnames = list(c("b1","b2"),c("a1","a2"),c("c1","c2"),c("d1","d2")))
names (dimnames (phi_XY)) <- c("B","A","D","C")
names (dimnames (psi)) <- c("B","A","D","C")
psi
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/zad3/zad3.R", echo=TRUE)
phi.Y
phi.B
phi.B
phi.X
phi.Y
psi <- array(0, dim = c(2,2,2,2), dimnames = list(c("b1","b2"),c("a1","a2"),c("d1","d2"),c("c1","c2")))
names (dimnames (psi)) <- c("B","A","D","C")
psi
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/lab4/lab4.R", echo=TRUE)
phi_XY[,,"no","no"]
phi_XY
psi
phi.X
phi.Y[,"d1","c1"]
phi.X
phi.X*phi.Y[,"d1","c1"]
psi[,,"d1","c1"] <- phi.X*phi.Y[,"d1","c1"]
psi
psi[,,"d2","c1"] <- phi.X*phi.Y[,"d2","c1"]
psi
psi[,,"d1","c2"] <- phi.X*phi.Y[,"d1","c2"]
psi[,,"d2","c2"] <- phi.X*phi.Y[,"d2","c2"]
psi
# Projekcja (marginalizacja) phi(A,S,T,L) wzglÄ™dem zmiennych L i T
apply (phi_ASTL, c("A","S"), sum)
apply (psi, c("A", "B"), sum)
psi.AB <- apply (psi, c("A", "B"), sum)
psi.AB/sum(psi.AB)
print("P(A,B)")
psi.AB/sum(psi.AB)
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/zad3/zad3.R", echo=TRUE)
print("P(A,B)")
psi.AB/sum(psi.AB)
P.AB <- psi.AB/sum(psi.AB)
P.AB
phi.B
phi.B
P.AB/phi.B
P.AB
phi.X
psi.AB/phi.B
P.AB
phi.X
phi.B
psi.AB
as.vector(psi.AB)
psi.AB/phi.B
as.vector(psi.AB)/phi.B
t(phi.B)
as.vector(psi.AB)/t(phi.B)
as.vector(t(psi.AB)/phi.B
as.vector(t(psi.AB))/phi.B
t(psi.AB)
phi.B
t(psi.AB)/phi.B
psi.AB <- apply (psi, c("B", "A"), sum)
P.AB <- psi.AB/sum(psi.AB)
print("P(A,B)")
P.AB
psi.AB
psi.AB/phi.B
P.A_B <- psi.AB/phi.B
P.A_B
P.A_B - phi.B
phi.B
P.A_B - phi.X
phi.X
P.A_B
round(P.A_B - phi.X)
round(P.A_B - phi.X, 2)
round(P.A_B - phi.X, 5)
print("phi.X")
phi.X
round(P.A_B - phi.X, 5)
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/zad3/zad3.R", echo=TRUE)
print("P(A|B) - phi.X")
round(P.A_B - phi.X, 5)
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/zad3/zad3.R", echo=TRUE)
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/zad3/zad3.R", echo=TRUE)
setwd("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/lab7")
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/lab7/lab7.R", echo=TRUE)
learn.net <- empty.graph(names(learning.test))
modelstring (learn.net) <- "[A][C][F][B|A][D|A:C][E|B:F]"
learn.net
learning.test
gaussian.test
names(learning.test)
learn.net
summary(learning.test)
plot(learn.net)
gauss.net <- empty.graph(names(gaussian.test))
modelstring(gauss.net) <- "[A][B][E][G][C|A:B][D|B][F|A:D:E:G]"
gauss.net
summary(gaussian.test)
plot(gauss.net)
score (learn.net, learning.test)
score (gauss.net, gaussian.test)
score (learn.net, learning.test, type = "aic")
score (learn.net, learning.test, type = "bde")
score (learn.net, learning.test, type = "bde", iss = 1)
score (learn.net, learning.test, type = "bde", iss = 3)
eq.net <- set.arc (gauss.net, "D", "B")
score (gauss.net, gaussian.test, type = "bic-g")
score (eq.net, gaussian.test, type = "bic-g")
all.equal (cpdag (gauss.net), cpdag (eq.net))
plot(eq.net)
eq.net <- set.arc (gauss.net, "F", "D")
score (eq.net, gaussian.test, type = "bic-g")
score (gauss.net, gaussian.test, type = "bic-g")
?cpdag
data (lizards)
ci.test ("Height", "Diameter", "Species", data = lizards, test = "mi")
lizards
summary(lizards)
ci.test ("Height", "Diameter", "Species", data = lizards, test = "mi")
ci.test ("Height", "Diameter", "Species", data = lizards)
ci.test ("Height", "Diameter", data = lizards)
ci.test ("Height", "Diameter", "Species", data = lizards, test = "sp-mi")
ci.test ("Diameter", "Height", "Species", data = lizards, test = "mc-x2")
ci.test ("A", "C", "B", data = gaussian.test, test = "cor")
ci.test ("A", "B", data = gaussian.test, test = "cor")
ci.test ("A", "G", data = gaussian.test, test = "cor")
ci.test ("A", "B", "C", data = gaussian.test, test = "cor")
plot (inter.iamb (learning.test))
plot(learn.net)
plot (inter.iamb (learning.test, blacklist = c("A", "B")))
bl <- matrix (c("A", "B", "B", "A"), ncol = 2, byrow = TRUE)
plot (inter.iamb (learning.test, blacklist = bl))
bl
pdag <- iamb (learning.test)
dag <- set.arc(pdag, from = "B", to = "A")
plot (dag)
dag <- pdag2dag (pdag, ordering = c("A", "B", "C", "D", "E", "F"))
plot (dag)
dag <- cextend(pdag)
plot (dag)
plot (hc (learning.test, blacklist = c("A", "B")))
plot (hc (learning.test, blacklist = bl))
plot (hc (learning.test, whitelist = c("A", "F")))
plot (tabu (learning.test, blacklist = c("A", "B")))
plot (hc (learning.test, whitelist = c("A", "F")))
pdag <- iamb (learning.test)
dag <- pdag2dag (pdag, ordering = c("A", "B", "C", "D", "E", "F"))
# Maximum likelihood estimator
learning.fit <- bn.fit (dag, learning.test, method = "mle")
# Bayesian posterior estimator
learning.fit <- bn.fit (dag, learning.test, method = "bayes")
# Hierarchical Dirichlet posterior estimator
learning.fit <- bn.fit (dag, learning.test, method = "hdir")
bn.fit.barchart (learning.fit$D)
learning.fit
dag <- hc (gaussian.test)
# Maximum likelihood estimator for least squares regression
gaussian.fit <- bn.fit (dag, gaussian.test, method = "mle-g")
gaussian.fit
dag <- iamb (gaussian.test)
plot(dag)
dag <- hc (gaussian.test)
pdag <- iamb (gaussian.test)
plot(pdag)
# Maximum likelihood estimator for least squares regression
gaussian.fit <- bn.fit (dag, gaussian.test, method = "mle-g")
gaussian.fit
# Maximum likelihood estimator for least squares regression
gaussian.fit <- bn.fit (pdag, gaussian.test, method = "mle-g")
gaussian.fit
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/zad6/zad6.R", echo=TRUE)
data("marks")
summary(marks)
head(marks, 6)
nrow(marks)
library(Rgrpahviz)
library(Rgraphviz)
?pc.stable
dag <- pc.stable(marks)
dag$learning
plot(pc.stable(marks, alpha = 0.03))
plot(pc.stable(marks, alpha = 0.07))
plot(pc.stable(marks, alpha = 0.08))
plot(pc.stable(marks, alpha = 0.07))
plot(pc.stable(marks))
plot(pc.stable(marks, alpha = 0.03))
plot(pc.stable(marks))
dag.pc <- pc.stable(marks)
plot(dag.pc)
dag.hc <- hc(marks)
plot(dag.hc)
plot(dag.pc)
plot(dag.hc)
# Odwrocone linki z Vect do ALG, z Stat do Alg, z Stat do Anl, z Anl do Alg
dag.hc$learning
?hc
dag.hc
dag.hc$learning
dag.hc
# 34 testy przy uczeniu, score = 2.238668
score(dag.hc, marks, type = "bic")
dag.hc$nodes
dag.hc
# 34 testy przy uczeniu, score = 2.238668
score(dag.hc, marks)
cpquery (dag.pc, event = (STAT > 60 & MECH > 60),
evidence = TRUE)
plot(dag.pc)
learning.fit <- bn.fit (dag.pc, marks)
# 34 testy przy uczeniu, score = -1731.407
dag.pc
dag.pc <- pc.stable(marks)
plot(dag.pc)
dag.hc
learning.fit <- bn.fit (dag.hc, marks)
cpquery (learning.fit, event = (STAT > 60 & MECH > 60),
evidence = TRUE)
cpquery (learning.fit, event = (STAT > 60 & MECH > 60),
evidence = (ALG>60))
score(dag.hc, marks)
cpquery (learning.fit, event = (STAT > 60),
evidence = (ALG>60))
cpquery (learning.fit, event = (STAT > 60),
evidence = (ALG>60 & MECH > 60))
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/zad6/zad6.R", echo=TRUE)
dag.pc$learning
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/zad6/zad6.R", echo=TRUE)
source("C:/Users/bogum/Desktop/mgr_studia/semestr4/Sieci_decyzyjne/zad6/zad6.R", echo=TRUE)
